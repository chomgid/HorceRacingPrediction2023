{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.0-cp310-cp310-win_amd64.whl (192.3 MB)\n",
      "     -------------------------------------- 192.3/192.3 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 9.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.4.0)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "     -------------------------------------- 166.4/166.4 KB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sympy in c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: networkx, fsspec, torch\n",
      "Successfully installed fsspec-2023.10.0 networkx-3.2 torch-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\wjswp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\wjswp\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.9348, -0.5668, -0.2168, -0.9017,  0.4590],\n",
      "        [ 1.2832,  0.5990,  0.2157,  2.8856, -0.2616],\n",
      "        [-0.1261,  0.7778,  0.2119,  1.2028,  0.2635]])\n",
      "tensor([[-1.9348, -0.5668, -0.2168, -0.9017,  0.4590],\n",
      "        [ 1.2832,  0.5990,  0.2157,  2.8856, -0.2616],\n",
      "        [-0.1261,  0.7778,  0.2119,  1.2028,  0.2635]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.randn(3,5)\n",
    "x_hat=torch.autograd.Variable(tensor,requires_grad=True)\n",
    "print(tensor)\n",
    "print(x_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mUntitled-2.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-2.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=1'>2</a>\u001b[0m x\u001b[39m=\u001b[39mVariable(torch\u001b[39m.\u001b[39mrandn(\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m),requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-2.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=2'>3</a>\u001b[0m out\u001b[39m=\u001b[39mx\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\u001b[39m+\u001b[39m\u001b[39m7\u001b[39m\u001b[39m*\u001b[39mx\u001b[39m+\u001b[39m\u001b[39m10\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:Untitled-2.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=3'>4</a>\u001b[0m out\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m      <a href='vscode-notebook-cell:Untitled-2.ipynb?jupyter-notebook#W2sdW50aXRsZWQ%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(out)\n",
      "File \u001b[1;32mc:\\Users\\wjswp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\wjswp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    235\u001b[0m inputs \u001b[39m=\u001b[39m (\n\u001b[0;32m    236\u001b[0m     (inputs,)\n\u001b[0;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    241\u001b[0m )\n\u001b[0;32m    243\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 244\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\wjswp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         )\n\u001b[0;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_floating_point:\n\u001b[0;32m    121\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    122\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(5):  # Critic updates (as recommended for WGAN)\n",
    "        # Generate random noise\n",
    "        z = torch.randn(batch_size, z_dim)\n",
    "        #batch_size, z_dim 차원의 정규분포를 따르는 랜덤 노이즈를 생성\n",
    "\n",
    "        # Generate fake data\n",
    "        fake_data = G(z)\n",
    "        #랜덤 노이즈를 generator에 집어넣어 가짜 데이터 생성\n",
    "        # Real data (not shown here: load and preprocess real data)\n",
    "\n",
    "        # Interpolate between real and fake data for gradient penalty\n",
    "        epsilon = torch.rand(batch_size, 1)\n",
    "        # epsilon생성 (추후에 보간법을 이용하여 가짜와 진짜 사이의 x값 생성)\n",
    "        x_hat = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "        #보간법을 이용한 x값 생성\n",
    "        x_hat = torch.autograd.Variable(x_hat, requires_grad=True)\n",
    "        #autograd.Variable를 통해 x_hat에서의 기울기 값 구함\n",
    "\n",
    "        # Compute critic scores for interpolated data\n",
    "        critic_scores_x_hat = D(x_hat)\n",
    "        #Discriminator에 x_hat를 넣어 critic_scores를 구함\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gradients = torch.autograd.grad(critic_scores_x_hat, x_hat, grad_outputs=torch.ones(critic_scores_x_hat.size()))\n",
    "        #Discriminator함수를 미분하고, 그 값에 x_hat를 넣겠다\n",
    "        #torch.ones()를 넣으면, 안에 critic_scores_x_hat.size() 차원의 1행렬이 나옴\n",
    "        #critic_scores_x_hat차원만큼의 gradient값이 나옴\n",
    "        gradient_penalty = lambda_gp * ((gradients[0].norm(2, dim=1) - 1) ** 2).mean()\n",
    "        #gradient_penalty를 구할 것인데, 각 기울기의 norm값에 -1를 한값의 평균에 lambda를 곱함\n",
    "        #각 행 grad값의 제곱의 합의 제곱근에 1를 뺴고, 제곱한 것의 평균을 구하고 lambda_gp를 곱합\n",
    "\n",
    "\n",
    "        # Update the discriminator\n",
    "        D_loss = -torch.mean(D(real_data)) + torch.mean(D(fake_data)) + gradient_penalty\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "    # Generate new random noise\n",
    "    z = torch.randn(batch_size, z_dim)\n",
    "\n",
    "    # Generate fake data\n",
    "    fake_data = G(z)\n",
    "\n",
    "    # Update the generator\n",
    "    G_loss = -torch.mean(D(fake_data))\n",
    "    G.zero_grad()\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] | D_loss: {D_loss.item()} | G_loss: {G_loss.item()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[66., 32., 66.],\n",
      "        [10., 32., 66.]])\n",
      "tensor(74363.0234)\n"
     ]
    }
   ],
   "source": [
    "#gradients = torch.autograd.grad(critic_scores_x_hat, x_hat, grad_outputs=torch.ones(critic_scores_x_hat.size()))\n",
    "input=torch.tensor([[3.0,2.0,3.0],[1.0,2.0,3.0]],requires_grad=True)\n",
    "output=2*input**3+2*input**2\n",
    "grads=torch.autograd.grad(output,input,grad_outputs=torch.ones(output.size()))\n",
    "gradient_penalty = 10 * ((grads[0].norm(2, dim=1) - 1) ** 2).mean()\n",
    "print(grads[0])\n",
    "print(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.67117106835208\n",
      "7436.30301922\n",
      "tensor([98.6712, 74.0270])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
